{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention_Example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "tloOTssMa7Mk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd9a99b0-0ee6-45db-a00e-9f992b95bc2e"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "class Attention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, key_dim=None, **kwargs):\n",
        "        \n",
        "        self.key_dim = key_dim\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "         # Weights initializer function\n",
        "        w_initializer = keras.initializers.glorot_uniform()\n",
        "\n",
        "        # Biases initializer function\n",
        "        b_initializer = keras.initializers.Zeros()\n",
        "        \n",
        "        #Matrix to extract the keys\n",
        "        self.key_extract = self.add_weight(name='feature_extract', \n",
        "                                      shape=(int(input_shape[2]),int(self.key_dim)),\n",
        "                                      initializer=w_initializer,\n",
        "                                      trainable=True)\n",
        "        #Key Bias\n",
        "        self.key_bias = self.add_weight(name='feaure_bias', \n",
        "                                      shape=(int(1),int(self.key_dim)),\n",
        "                                      initializer=b_initializer,\n",
        "                                      trainable=True)\n",
        "        \n",
        "        #The Query representing the class\n",
        "        self.Query = self.add_weight(name='Query', \n",
        "                                      shape=(int(self.key_dim),int(1)),\n",
        "                                      initializer=w_initializer,\n",
        "                                      trainable=True)\n",
        "\n",
        "        super(Attention, self).build(input_shape) \n",
        "\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        #Extract the Keys\n",
        "        keys=tf.tensordot(x,self.key_extract,axes=[2,0])+self.key_bias\n",
        "        \n",
        "        #Calculate the similarity between keys and the Query\n",
        "        similar_logits=tf.tensordot(keys,self.Query,axes=[2,0])\n",
        "        \n",
        "        #Normalize it to be between 0 and 1 and sum to 1\n",
        "        attention_weights = tf.nn.softmax(similar_logits,axis=1)\n",
        "        \n",
        "        #Use these Weights to aggregate\n",
        "        weighted_input = tf.matmul(x, attention_weights, transpose_a=True)\n",
        "\n",
        "        \n",
        "        return weighted_input\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0],input_shape[2],int(1))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8CLk1_wImtlY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YH6ah9fAjjIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "class Self_Attention(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, key_dim=None, **kwargs):\n",
        "        \n",
        "        self.key_dim = key_dim\n",
        "        super(Self_Attention, self).__init__(**kwargs)\n",
        "\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "         # Weights initializer function\n",
        "        w_initializer = keras.initializers.glorot_uniform()\n",
        "\n",
        "        # Biases initializer function\n",
        "        b_initializer = keras.initializers.Zeros()\n",
        "        \n",
        "        #Matrix to extract the keys\n",
        "        self.key_extract = self.add_weight(name='feature_extract', \n",
        "                                      shape=(int(input_shape[2]),int(self.key_dim)),\n",
        "                                      initializer=w_initializer,\n",
        "                                      trainable=True)\n",
        "        #Key Bias\n",
        "        self.key_bias = self.add_weight(name='feaure_bias', \n",
        "                                      shape=(int(1),int(self.key_dim)),\n",
        "                                      initializer=b_initializer,\n",
        "                                      trainable=True)\n",
        "        \n",
        "        #The Query representing the class\n",
        "        self.query_extract = self.add_weight(name='q_extract', \n",
        "                                      shape=(int(input_shape[2]),int(self.key_dim)),\n",
        "                                      initializer=w_initializer,\n",
        "                                      trainable=True)\n",
        "        self.query_bias = self.add_weight(name='q_bias', \n",
        "                              shape=(int(1),int(self.key_dim)),\n",
        "                              initializer=b_initializer,\n",
        "                              trainable=True)\n",
        "\n",
        "        super(Self_Attention, self).build(input_shape) \n",
        "\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        #Extract the Keys\n",
        "        keys=tf.tensordot(x,self.key_extract,axes=[2,0])+self.key_bias\n",
        "        #Extract the Keys\n",
        "        query=tf.tensordot(x,self.query_extract,axes=[2,0])+self.query_bias\n",
        "        \n",
        "        #Calculate the similarity between keys and the Query\n",
        "        similar_logits=tf.matmul(query,keys,transpose_b=True)\n",
        "        \n",
        "        #Normalize it to be between 0 and 1 and sum to 1\n",
        "        attention_weights = tf.nn.softmax(similar_logits,axis=1)\n",
        "        \n",
        "        #Use these Weights to aggregate\n",
        "        weighted_input = tf.matmul(attention_weights, x)\n",
        "\n",
        "        \n",
        "        return weighted_input\n",
        "\n",
        "#    def compute_output_shape(self, input_shape):\n",
        "#        return (input_shape[0],input_shape[1],input_shape[2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FXlDsxJqhvrg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "def Multi_Attention(in_feature,N_Heads,key_dim=100): \n",
        "  splits=Lambda(lambda x : tf.split(x,axis=2,num_or_size_splits=N_Heads))(in_feature)\n",
        "  feature_List=[]\n",
        "  for elem in splits:\n",
        "    feature_List.append(Self_Attention(key_dim)(elem))\n",
        "  return Concatenate(axis=2)(feature_List)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BTRWOpXXlvxL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import Dense, Dropout, Activation, Input,LSTM,Lambda,Concatenate\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d-IofDAil2Nu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Inpu=Input(shape=(3,))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xRCAvFjjl3_X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Embeddings=Embedding(output_dim=100,input_dim=10000)(Inpu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nxuIQaH8l574",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Features=Multi_Attention(Embeddings,N_Heads=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uxxY-l3HmIom",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Aggregation=Attention(100)(Features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LjQsQabvrhDT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Aggregation=keras.layers.Flatten()(Aggregation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PxCz5L2sl_zK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Prediction=Dense(3,activation=\"softmax\")(Aggregation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_hmMN-utmGL1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=Model(inputs=Inpu,outputs=Prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bXrNfFw2oIL-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "b28b712b-5a91-4bad-e98c-225b1ae3aa0d"
      },
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 3)                 0         \n",
            "_________________________________________________________________\n",
            "embedding_3 (Embedding)      (None, 3, 100)            1000000   \n",
            "_________________________________________________________________\n",
            "model_3 (Model)              multiple                  22000     \n",
            "_________________________________________________________________\n",
            "attention_3 (Attention)      (None, 100, 1)            10200     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 303       \n",
            "=================================================================\n",
            "Total params: 1,032,503\n",
            "Trainable params: 1,032,503\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8GqsO34koT2H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##Example \"sub. Model\"\n",
        "\n",
        "def Create_Feature_Model(in_features,N_Heads):\n",
        "  Inpu=Input(shape=(None,in_features))\n",
        "  Features=Multi_Attention(Inpu,N_Heads=N_Heads)\n",
        "  return Model(inputs=Inpu,outputs=Features,name=\"Multi_Head_Attention\",)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WyBGCcNhlx7h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_self_a=Create_Feature_Model(in_features=100,N_Heads=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d0G_gHyWl3eo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}